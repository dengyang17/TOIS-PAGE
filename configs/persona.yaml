# Data
#data: '../data/Electronics/'
max_time_step: 150
shared_vocab: True
unk: True
label_smoothing: 0.1
scale: 1 # Proportion of the training set
refF: ''


# Logging
logdir: 'experiments/'
report_interval: 100
eval_interval: 4
save_interval: 4



# Optimization
epoch: 20
optim: 'adam'
learning_rate: 2
learning_rate_decay: 0.95
start_decay_steps: 10000
decay_method: "noam"
beta1: 0.9
beta2: 0.998
max_grad_norm: 1
warmup_steps: 16000
epoch_decay: False # decay by epochs after decay starts
schedule: False # Learning rate schedule
schesamp: False # Scheduled sampling


# Model
model: 'tensor2tensor'
## Transformer
positional: True
heads: 8
d_ff: 2048
## RNN
cell: 'lstm'
convolutional: False
bidirectional: True
char: False # character-level encoding
## 
param_init: 0
param_init_glorot: True
emb_size: 512
hidden_size: 512
enc_num_layers: 6
dec_num_layers: 2
dropout: 0.1
emb_dropout: 0.1
swish: False
length_norm: True
pool_size: 0 # Pool size of maxout layer
topic_num: 10
bow_vocab_size: 10000


# Others
seed: 1234
use_cuda: True
Bernoulli: False # Bernoulli selection
gate: False # To guarantee selfatttn is working for global encoding
selfatt: False # selfatt for both global encoding and inverse attention
label_dict_file: ''

## KOBE
persona: True
pointer: True
vae: True
copy: True
experience: True